# train_ml_global_v2.py
"""
–û–±—É—á–µ–Ω–∏–µ LightGBM –º–æ–¥–µ–ª–∏ —Å –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ–º –¥–∞–Ω–Ω—ã—Ö –∏–∑ ml_labeling_tool_v3.py
–∏ –∏–Ω–¥–∏–∫–∞—Ç–æ—Ä–æ–≤ –∏–∑ MarketDataUtils

–ê–≤—Ç–æ—Ä: pwm777
–î–∞—Ç–∞: 2025-10-25 (–æ–±–Ω–æ–≤–ª–µ–Ω–æ)
–í–µ—Ä—Å–∏—è: 2.0 (–∏–Ω—Ç–µ–≥—Ä–∞—Ü–∏—è —Å labeling tool)

–û–ø–∏—Å–∞–Ω–∏–µ:
- –ò—Å–ø–æ–ª—å–∑—É–µ—Ç SQLite –±–∞–∑—É –¥–∞–Ω–Ω—ã—Ö –∏–∑ ml_labeling_tool_v3.py
- –í—Å–µ –ø—Ä–∏–∑–Ω–∞–∫–∏ —Ä–∞—Å—Å—á–∏—Ç—ã–≤–∞—é—Ç—Å—è –ø–æ —Ç–µ–º –∂–µ —Ñ–æ—Ä–º—É–ª–∞–º, —á—Ç–æ –∏ –≤ –±–æ–µ–≤–æ–º –±–æ—Ç–µ
- –ê–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∞—è –∑–∞–≥—Ä—É–∑–∫–∞ –¥–∞–Ω–Ω—ã—Ö –∏–∑ market_data.sqlite
- –ü–æ–¥–¥–µ—Ä–∂–∫–∞ –±–∞–ª–∞–Ω—Å–∏—Ä–æ–≤–∫–∏ –¥–∞—Ç–∞—Å–µ—Ç–∞ –∫–∞–∫ –≤ labeling tool

–ó–∞–ø—É—Å–∫:
    python train_ml_global_v2.py
"""

import sys
import logging
from sqlalchemy import create_engine, text
from datetime import datetime
import json
from typing import Tuple
import warnings
import lightgbm as lgb
from sklearn.metrics import accuracy_score, precision_recall_fscore_support, confusion_matrix
from collections import Counter
import joblib
from pathlib import Path

warnings.filterwarnings('ignore')
import re
import os, numpy as np, pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.preprocessing import label_binarize, StandardScaler
from sklearn.metrics import precision_recall_curve, average_precision_score

TIMEFRAME_TO_BARS = {"1m": 1440, "3m": 480, "5m": 288, "15m": 96, "30m": 48, "1h": 24}

# –ù–∞—Å—Ç—Ä–æ–π–∫–∞ –ª–æ–≥–∏—Ä–æ–≤–∞–Ω–∏—è
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(levelname)s - %(message)s',
    stream=sys.stdout
)
logger = logging.getLogger(__name__)

DATA_DIR = Path("data")
DATA_DIR.mkdir(exist_ok=True)
MARKET_DB_DSN: str = f"sqlite:///{DATA_DIR}/market_data.sqlite"
IGNORE_CLASS_3 = False  # ‚Üê –æ—Ç–∫–ª—é—á–∞–µ–º: –∫–ª–∞—Å—Å 3 –æ–±—ä–µ–¥–∏–Ω—è–µ—Ç—Å—è —Å 0
MERGE_CLASS_3_TO_0 = True  # ‚Üê –∞–∫—Ç–∏–≤–Ω—ã–π —Ä–µ–∂–∏–º: 3‚Üí0
# ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
# –°–ü–ò–°–û–ö –ü–†–ò–ó–ù–ê–ö–û–í (—Å–æ–≤–º–µ—Å—Ç–∏–º—ã–π —Å labeling tool)
# ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
feature_names = [
    'cmo_14',
    'volume',
    'trend_acceleration_ema7',
    'regime_volatility',
    'bb_width',
    'adx_14',
    'plus_di_14',  # ‚Üê –î–û–ë–ê–í–õ–ï–ù
    'minus_di_14',
    'atr_14_normalized',
    'volume_ratio_ema3',
    'candle_relative_body',
    'upper_shadow_ratio',
    'lower_shadow_ratio',
    'price_vs_vwap',
    'bb_position',
    'cusum_1m_recent',
    'cusum_1m_quality_score',
    'cusum_1m_trend_aligned',
    'cusum_1m_price_move',
    'is_trend_pattern_1m',
    'body_to_range_ratio_1m',
    'close_position_in_range_1m'
]


# ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
# –ö–õ–ê–°–° –î–õ–Ø –†–ê–ë–û–¢–´ –° –ë–ê–ó–û–ô –î–ê–ù–ù–´–•
# ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
def _infer_bars_per_day_from_run_id(run_id: str, default: int = 288) -> int:
    """
    –ü—ã—Ç–∞–µ—Ç—Å—è –≤—ã—Ç–∞—â–∏—Ç—å —Ç–∞–π–º—Ñ—Ä–µ–π–º –∏–∑ run_id —Ñ–æ—Ä–º–∞—Ç–∞ ..._<tf>_...
    –∏ –≤–µ—Ä–Ω—É—Ç—å —Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤—É—é—â–µ–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ –±–∞—Ä–æ–≤ –≤ —Å—É—Ç–∫–∞—Ö.
    """
    m = re.search(r"_(\d+[mh])_", str(run_id).lower())
    if not m:
        return default
    tf = m.group(1)
    TIMEFRAME_TO_BARS = {
        "1m": 1440,
        "3m": 480,
        "5m": 288,
        "15m": 96,
        "30m": 48,
        "1h": 24,
        "2h": 12,
        "4h": 6,
        "6h": 4,
        "12h": 2,
        "1d": 1,
    }
    return TIMEFRAME_TO_BARS.get(tf, default)


class DataLoader:
    """–ó–∞–≥—Ä—É–∑–∫–∞ –¥–∞–Ω–Ω—ã—Ö –∏–∑ SQLite –±–∞–∑—ã ml_labeling_tool_v2.py"""

    def __init__(self, db_dsn: str = MARKET_DB_DSN, symbol: str = "ETHUSDT"):
        self.db_dsn = db_dsn
        self.db_path = DATA_DIR / "market_data.sqlite"
        self.symbol = symbol
        self.engine = None

    def connect(self):
        """–£—Å—Ç–∞–Ω–æ–≤–∫–∞ —Å–æ–µ–¥–∏–Ω–µ–Ω–∏—è —Å –ë–î"""
        if not self.db_path.exists():
            raise FileNotFoundError(f"–ë–∞–∑–∞ –¥–∞–Ω–Ω—ã—Ö –Ω–µ –Ω–∞–π–¥–µ–Ω–∞: {self.db_path}")
        self.engine = create_engine(self.db_dsn)
        logger.info(f"‚úÖ –ü–æ–¥–∫–ª—é—á–µ–Ω–æ –∫ –ë–î: {self.db_path}")

    def close(self):
        """–ó–∞–∫—Ä—ã—Ç–∏–µ —Å–æ–µ–¥–∏–Ω–µ–Ω–∏—è"""
        if self.engine:
            self.engine.dispose()
            logger.info("‚úÖ –°–æ–µ–¥–∏–Ω–µ–Ω–∏–µ —Å –ë–î –∑–∞–∫—Ä—ã—Ç–æ")

    def load_market_data(self) -> pd.DataFrame:
        """–ó–∞–≥—Ä—É–∑–∫–∞ —Å–≤–µ—á–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö –∏–∑ candles_5m"""
        if not self.engine:
            self.connect()

        query = text("""
            SELECT * FROM candles_5m 
            WHERE symbol = :symbol 
            ORDER BY ts
        """)

        with self.engine.connect() as conn:
            df = pd.read_sql_query(query, conn, params={"symbol": self.symbol})

        if df.empty:
            raise ValueError(f"–ù–µ—Ç –¥–∞–Ω–Ω—ã—Ö –¥–ª—è —Å–∏–º–≤–æ–ª–∞ {self.symbol}")

        # –ü—Ä–µ–æ–±—Ä–∞–∑–æ–≤–∞–Ω–∏–µ –≤—Ä–µ–º–µ–Ω–∏
        if 'ts' in df.columns and 'datetime' not in df.columns:
            df['datetime'] = pd.to_datetime(df['ts'], unit='ms')

        logger.info(f"‚úÖ –ó–∞–≥—Ä—É–∂–µ–Ω–æ {len(df)} —Å–≤–µ—á–µ–π –∏–∑ candles_5m")
        return df

    def load_training_dataset(self, run_id: str) -> pd.DataFrame:
        """–ó–∞–≥—Ä—É–∑–∫–∞ –≥–æ—Ç–æ–≤–æ–≥–æ –¥–∞—Ç–∞—Å–µ—Ç–∞ –∏–∑ training_dataset"""
        if not self.engine:
            self.connect()

        query = text("""
            SELECT * FROM training_dataset
            WHERE run_id = :run_id
            ORDER BY ts
        """)

        with self.engine.connect() as conn:
            df = pd.read_sql_query(query, conn, params={"run_id": run_id})

        if df.empty:
            raise ValueError(f"‚ùå –ù–µ—Ç –¥–∞–Ω–Ω—ã—Ö –¥–ª—è run_id={run_id}")

        logger.info(f"‚úÖ –ó–∞–≥—Ä—É–∂–µ–Ω–æ {len(df)} –æ–±—Ä–∞–∑—Ü–æ–≤ –∏–∑ training_dataset")
        logger.info(f"   –ö–ª–∞—Å—Å—ã: {df['reversal_label'].value_counts().to_dict()}")
        return df


# ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
# –ö–û–õ–õ–ë–≠–ö ¬´–¢–ï–†–ú–û–ú–ï–¢–† –ü–†–û–ì–†–ï–°–°–ê¬ª –î–õ–Ø LIGHTGBM
# ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ

def thermometer_progress_callback(logger: logging.Logger, width: int = 30, period: int = 10):
    """–ü—Ä–æ–≥—Ä–µ—Å—Å-–±–∞—Ä –ø–æ –∏—Ç–µ—Ä–∞—Ü–∏—è–º –±—É—Å—Ç–∏–Ω–≥–∞"""
    import sys

    def _cb(env):
        begin = getattr(env, 'begin_iteration', 0) or 0
        end = getattr(env, 'end_iteration', None)
        if end is None or end <= begin:
            end = begin + int(env.params.get('num_boost_round', 0) or 0)
            if end <= begin:
                end = begin + 1

        total = max(1, end - begin)
        iter_now = int(getattr(env, 'iteration', 0) or 0)
        done = max(0, iter_now - begin + 1)
        pct = min(1.0, max(0.0, done / total))
        filled = int(round(pct * width))
        bar = '‚ñà' * filled + '‚ñë' * (width - filled)

        # –ú–µ—Ç—Ä–∏–∫–∏ –≤–∞–ª–∏–¥–∞—Ü–∏–∏
        val_metric_name = None
        val_metric_value = None
        evals = getattr(env, 'evaluation_result_list', None)
        if evals:
            for res in evals:
                if isinstance(res, (list, tuple)) and len(res) >= 3:
                    data_name, metric_name, metric_val = res[0], res[1], res[2]
                    if str(data_name).startswith(('valid', 'val')):
                        val_metric_name = str(metric_name)
                        try:
                            val_metric_value = float(metric_val)
                        except Exception:
                            val_metric_value = None
                        break

        should_print = (iter_now > 0 and iter_now % period == 0) or (done >= total)

        if should_print:
            if val_metric_name is not None and val_metric_value is not None:
                msg = f"[{iter_now:4d}/{total}] {val_metric_name}:{val_metric_value:.5f} | {bar} {int(pct * 100):3d}%"
            else:
                msg = f"[{iter_now:4d}/{total}] {bar} {int(pct * 100):3d}%"

            if done >= total:
                print(f"\r{msg}")
                sys.stdout.flush()
            else:
                print(f"\r{msg}", end='', flush=True)

    return _cb


# ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
# –ì–õ–ê–í–ù–´–ô –ö–õ–ê–°–° ModelTrainer
# ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ

class ModelTrainer:
    def __init__(self, db_dsn: str, symbol: str):
        self.db_dsn = db_dsn
        self.symbol = symbol
        self.data_loader = DataLoader(db_dsn, symbol)
        self.feature_names = feature_names

    def prepare_training_data(self, run_id: str) -> Tuple[pd.DataFrame, pd.Series, pd.Series]:
        """–ü–æ–¥–≥–æ—Ç–æ–≤–∫–∞ –¥–∞–Ω–Ω—ã—Ö –∏–∑ training_dataset"""
        df = self.data_loader.load_training_dataset(run_id)

        # –§–∏—á–∏ —É–∂–µ –≤ –¥–∞—Ç–∞—Å–µ—Ç–µ
        X = df[self.feature_names]
        y = df['reversal_label']
        w = df['sample_weight']

        # –ü—Ä–æ–≤–µ—Ä–∫–∞ –ø—Ä–æ–ø—É—Å–∫–æ–≤
        missing = X.isnull().sum()
        if missing.any():
            logger.warning(f"‚ö†Ô∏è  –û–±–Ω–∞—Ä—É–∂–µ–Ω—ã –ø—Ä–æ–ø—É—Å–∫–∏:\n{missing[missing > 0]}")
            X = X.fillna(0)

        logger.info(f"‚úÖ –ü–æ–¥–≥–æ—Ç–æ–≤–ª–µ–Ω—ã –¥–∞–Ω–Ω—ã–µ: {len(X)} –ø—Ä–∏–º–µ—Ä–æ–≤, {len(self.feature_names)} –ø—Ä–∏–∑–Ω–∞–∫–æ–≤")
        logger.info(f"   –†–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ –∫–ª–∞—Å—Å–æ–≤: {y.value_counts().to_dict()}")

        return X, y, w

    def tune_tau_for_spd_range(
            self,
            y_val: np.ndarray,
            proba: np.ndarray,  # shape (n,3) ‚Äî [HOLD, BUY, SELL]
            bars_per_day: int,
            spd_min: float = 30.0,
            spd_max: float = 50.0,
            precision_min: float = 0.60,
            delta: float = 0.08,
            cooldown_bars: int = 2,
    ):
        """
        –ü–æ–¥–±–∏—Ä–∞–µ—Ç tau (–ø–æ—Ä–æ–≥ max(p_buy,p_sell)) —Ç–∞–∫, —á—Ç–æ–±—ã SPD –±—ã–ª –≤ [spd_min, spd_max].
        –í–æ–∑–≤—Ä–∞—â–∞–µ—Ç (tau, stats_dict). –ï—Å–ª–∏ –Ω–µ –Ω–∞–π–¥–µ–Ω–æ ‚Äî –±–µ—Ä—ë—Ç tau —Å SPD –±–ª–∏–∂–∞–π—à–∏–º –∫ —Ü–µ–Ω—Ç—Ä—É –¥–∏–∞–ø–∞–∑–æ–Ω–∞.
        """
        # –ø—Ä–µ–¥—Ä–∞—Å—á—ë—Ç—ã
        p_buy, p_sell = proba[:, 1], proba[:, 2]
        maxp = np.maximum(p_buy, p_sell)
        taus = np.quantile(maxp, np.linspace(0.30, 0.95, 48))

        best_in = None  # (cand, key)  ‚Äî –ª—É—á—à–∏–π –≤–Ω—É—Ç—Ä–∏ –¥–∏–∞–ø–∞–∑–æ–Ω–∞
        best_near = None  # (cand, keyn) ‚Äî –±–ª–∏–∂–∞–π—à–∏–π –∫ —Ü–µ–Ω—Ç—Ä—É –¥–∏–∞–ø–∞–∑–æ–Ω–∞
        target = 0.5 * (spd_min + spd_max)
        n = len(y_val)

        for tau_cand in sorted(taus):
            # –µ–¥–∏–Ω—ã–π —Ä–∞—Å—á—ë—Ç –º–µ—Ç—Ä–∏–∫/act —á–µ—Ä–µ–∑ helper
            stats = self._eval_decision_metrics(
                y_true=np.asarray(y_val),
                proba=np.asarray(proba),
                tau=float(tau_cand),
                delta=float(delta),
                cooldown_bars=int(cooldown_bars),
                bars_per_day=int(bars_per_day),
            )
            spd = stats['spd']
            prec = stats['precision_macro_buy_sell']
            rec = stats['recall_macro_buy_sell']
            f1 = stats['f1_macro_buy_sell']
            # –≤–æ—Å—Å—Ç–∞–Ω–æ–≤–∏–º –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ —Å–∏–≥–Ω–∞–ª–æ–≤ –∏–∑ SPD (–ø–æ—Å–ª–µ cooldown)
            signals = int(round(spd * max(1, n) / max(1, bars_per_day)))

            cand = (float(tau_cand), float(spd), float(prec), float(rec), float(f1), int(signals))

            # –≤–Ω—É—Ç—Ä–∏ –¥–∏–∞–ø–∞–∑–æ–Ω–∞ ‚Äî –º–∞–∫—Å–∏–º–∏–∑–∏—Ä—É–µ–º precision, –∑–∞—Ç–µ–º F1, –∑–∞—Ç–µ–º –±–ª–∏–∂–µ –∫ —Ü–µ–Ω—Ç—Ä—É, –∑–∞—Ç–µ–º –±–æ–ª—å—à–∏–π œÑ
            if spd_min <= spd <= spd_max and prec >= precision_min:
                key = (prec, f1, -abs(spd - target), float(tau_cand))
                if (best_in is None) or (key > best_in[1]):
                    best_in = (cand, key)

            # –±–ª–∏–∂–∞–π—à–∏–π –∫ —Ü–µ–Ω—Ç—Ä—É ‚Äî –Ω–∞ —Å–ª—É—á–∞–π, –µ—Å–ª–∏ –≤ –¥–∏–∞–ø–∞–∑–æ–Ω–µ –Ω–µ—Ç –ø–æ–¥—Ö–æ–¥—è—â–∏—Ö
            gap = abs(spd - target)
            keyn = (-gap, prec, f1)  # –º–∏–Ω–∏–º–∏–∑–∏—Ä—É–µ–º gap, –∑–∞—Ç–µ–º –º–∞–∫—Å. precision/F1
            if (best_near is None) or (keyn > best_near[1]):
                best_near = (cand, keyn)

        # –æ—Å–Ω–æ–≤–Ω–æ–π –≤—ã–±–æ—Ä
        chosen = (best_in[0] if best_in is not None else best_near[0])
        tau_chosen, spd, prec, rec, f1, signals = chosen

        # ‚îÄ‚îÄ –õ–æ–∫–∞–ª—å–Ω—ã–π –ø–æ–¥—ä—ë–º œÑ –≤–≤–µ—Ä—Ö (–µ—Å–ª–∏ –Ω–µ —Ö—É–∂–µ –∏ –æ—Å—Ç–∞—ë–º—Å—è –≤ —Ç–µ–∫—É—â–µ–º –æ–∫–Ω–µ SPD) ‚îÄ‚îÄ
        upper = min(float(tau_chosen) + 0.05, 0.999)
        ref_grid = np.linspace(float(tau_chosen), upper, 31)  # —à–∞–≥ ‚âà0.0017

        best_ref = None  # (key_ref, t, stats_ref)
        for t in ref_grid:
            stats_ref = self._eval_decision_metrics(
                y_true=np.asarray(y_val),
                proba=np.asarray(proba),
                tau=float(t),
                delta=float(delta),
                cooldown_bars=int(cooldown_bars),
                bars_per_day=int(bars_per_day),
            )
            spd_r = stats_ref['spd']
            prec_r = stats_ref['precision_macro_buy_sell']
            f1_r = stats_ref['f1_macro_buy_sell']

            # –ø—Ä–∏–æ—Ä–∏—Ç–µ—Ç –∫–∞—á–µ—Å—Ç–≤–∞; –æ–∫–Ω–æ SPD –∏—Å–ø–æ–ª—å–∑—É–µ–º —Ç–µ–∫—É—â–µ–µ (spd_min..spd_max) –∏ —Ç–µ–∫—É—â–∏–π precision_min
            if (spd_min <= spd_r <= spd_max) and (prec_r >= precision_min):
                # –∫–ª—é—á: –º–∞–∫—Å F1, –∑–∞—Ç–µ–º –±–æ–ª—å—à–∏–π œÑ, –∑–∞—Ç–µ–º –±–ª–∏–∂–µ –∫ —Ü–µ–Ω—Ç—Ä—É SPD
                key_ref = (f1_r, float(t), -abs(spd_r - target))
                if (best_ref is None) or (key_ref > best_ref[0]):
                    best_ref = (key_ref, float(t), stats_ref)

        # –ø—Ä–∏–º–µ–Ω—è–µ–º —É–ª—É—á—à–µ–Ω–∏–µ, –µ—Å–ª–∏ –Ω–∞–π–¥–µ–Ω–æ
        if best_ref is not None:
            _, tau_new, sref = best_ref
            tau_chosen = tau_new
            spd = float(sref['spd'])
            prec = float(sref['precision_macro_buy_sell'])
            rec = float(sref['recall_macro_buy_sell'])
            f1 = float(sref['f1_macro_buy_sell'])
            signals = int(round(spd * max(1, len(y_val)) / max(1, bars_per_day)))

        # hit_range –¥–æ–ª–∂–µ–Ω –æ—Ç—Ä–∞–∂–∞—Ç—å —Ñ–∞–∫—Ç –ø–æ–ø–∞–¥–∞–Ω–∏—è –ò–¢–û–ì–û–í–û–ì–û –≤—ã–±–æ—Ä–∞ –≤ –æ–∫–Ω–æ –∏ –ø–æ precision
        in_range = (spd_min <= spd <= spd_max) and (prec >= precision_min)

        return float(tau_chosen), {
            "spd": float(spd),
            "precision_macro_buy_sell": float(prec),
            "recall_macro_buy_sell": float(rec),
            "f1_macro_buy_sell": float(f1),
            "signals": int(signals),
            "delta": float(delta),
            "cooldown_bars": int(cooldown_bars),
            "range": [float(spd_min), float(spd_max)],
            "hit_range": bool(in_range),
        }

    @staticmethod
    def _eval_decision_metrics(y_true: np.ndarray,
                               proba: np.ndarray,  # shape (n,3) [HOLD, BUY, SELL]
                               tau: float,
                               delta: float,
                               cooldown_bars: int,
                               bars_per_day: int) -> dict:
        """
        –£–Ω–∏—Ñ–∏—Ü–∏—Ä–æ–≤–∞–Ω–Ω—ã–π —Ä–∞—Å—á—ë—Ç act/–ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–π/–º–µ—Ç—Ä–∏–∫, –∏—Å–ø–æ–ª—å–∑—É–µ–º—ã–π –∏ –≤ —Ç—é–Ω–µ—Ä–µ, –∏ –≤ sensitivity.
        –í–ê–ñ–ù–û: –º–µ—Ç—Ä–∏–∫–∏ —Å—á–∏—Ç–∞—é—Ç—Å—è –¢–û–õ–¨–ö–û –Ω–∞ –∏–Ω–¥–µ–∫—Å–µ act=True (–∫–∞–∫ –≤ —Ç—é–Ω–µ—Ä–µ), labels=[1,2].
        """
        p_buy = proba[:, 1]
        p_sell = proba[:, 2]
        maxp = np.maximum(p_buy, p_sell)
        margin = np.abs(p_buy - p_sell)

        act = (maxp >= tau) & (margin >= delta)

        # cooldown –ø–æ –∏–Ω–¥–µ–∫—Å–∞–º —Å—Ä–∞–±–∞—Ç—ã–≤–∞–Ω–∏–π
        idx = np.where(act)[0]
        if idx.size > 0:
            keep = [idx[0]]
            for i in idx[1:]:
                if i - keep[-1] >= cooldown_bars:
                    keep.append(i)
            sel = np.zeros_like(act, dtype=bool)
            sel[np.array(keep, dtype=int)] = True
            act = sel

        # –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è 0/1/2 –ø–æ –ø–æ–ª–∏—Ç–∏–∫–µ
        pred = np.zeros(len(proba), dtype=int)  # HOLD=0
        buy_ge_sell = p_buy >= p_sell
        pred[act & buy_ge_sell] = 1
        pred[act & (~buy_ge_sell)] = 2

        # SPD
        spd_val = act.sum() * bars_per_day / max(1, len(y_true))

        # –º–µ—Ç—Ä–∏–∫–∏ –Ω–∞ –∞–∫—Ç–∏–≤–Ω–æ–º –ø–æ–¥–º–Ω–æ–∂–µ—Å—Ç–≤–µ (–∫–∞–∫ –≤ —Ç—é–Ω–µ—Ä–µ)
        if np.any(act):
            pm, rm, fm, _ = precision_recall_fscore_support(
                y_true[act], pred[act], labels=[1, 2], average='macro', zero_division=0
            )
        else:
            pm = rm = fm = 0.0

        return {
            'spd': float(spd_val),
            'precision_macro_buy_sell': float(pm),
            'recall_macro_buy_sell': float(rm),
            'f1_macro_buy_sell': float(fm),
            # –¥—É–±–ª–∏—Ä—É–µ–º –¥–ª—è —É–¥–æ–±—Å—Ç–≤–∞
            'tau': float(tau),
            'delta': float(delta),
            'cooldown_bars': int(cooldown_bars),
        }

    @staticmethod
    def decide(proba, tau, delta=0.08, cooldown_bars=2):
        p_buy, p_sell = proba[:, 1], proba[:, 2]
        maxp = np.maximum(p_buy, p_sell)
        margin = np.abs(p_buy - p_sell)
        act = (maxp >= tau) & (margin >= delta)

        # Apply cooldown
        idx = np.where(act)[0]
        if idx.size > 0:
            keep = [idx[0]]
            for i in idx[1:]:
                if i - keep[-1] >= cooldown_bars:
                    keep.append(i)
            sel = np.zeros_like(act, dtype=bool)
            sel[np.array(keep, dtype=int)] = True
            act = sel

        pred = np.zeros(len(proba), dtype=int)
        pred[act] = np.where(p_buy[act] >= p_sell[act], 1, 2)
        return pred

    def train_model(self, run_id: str, use_scaler: bool = True) -> dict:
        """–û–±—É—á–µ–Ω–∏–µ –º–æ–¥–µ–ª–∏ (–≤–∞—Ä–∏–∞–Ω—Ç A: –∏–≥–Ω–æ—Ä–∏—Ä—É–µ–º –∫–ª–∞—Å—Å 3 –ø–æ–ª–Ω–æ—Å—Ç—å—é)"""

        logger.info("\n" + "=" * 60)
        logger.info("–û–ë–£–ß–ï–ù–ò–ï –ú–û–î–ï–õ–ò LIGHTGBM")
        logger.info("=" * 60)

        # –ü–æ–¥–≥–æ—Ç–æ–≤–∫–∞ –¥–∞–Ω–Ω—ã—Ö
        X, y, w = self.prepare_training_data(run_id)

        # –†–∞–∑–¥–µ–ª–µ–Ω–∏–µ –Ω–∞ train/val –ø–æ –≤—Ä–µ–º–µ–Ω–∏ (80/20)
        split_idx = int(len(X) * 0.8)
        X_train, X_val = X.iloc[:split_idx], X.iloc[split_idx:]
        y_train, y_val = y.iloc[:split_idx], y.iloc[split_idx:]
        w_train, w_val = w.iloc[:split_idx], w.iloc[split_idx:]

        # --- –ü–æ–ª–Ω–æ—Å—Ç—å—é –∏—Å–∫–ª—é—á–∞–µ–º —Å–ª—É–∂–µ–±–Ω—ã–π –∫–ª–∞—Å—Å 3 –∏–∑ –æ–±—É—á–µ–Ω–∏—è –∏ –≤–∞–ª–∏–¥–∞—Ü–∏–∏ ---
        mask_tr = (y_train != 3)
        mask_va = (y_val != 3)

        X_train = X_train[mask_tr]
        y_train = y_train[mask_tr]
        w_train = w_train[mask_tr]

        X_val = X_val[mask_va]
        y_val = y_val[mask_va]
        w_val = w_val[mask_va]

        NUM_CLASS = 3
        REPORT_LABELS = [1, 2, 0]  # –ü–æ—Ä—è–¥–æ–∫ –º–µ—Ç–æ–∫ –¥–ª—è –æ—Ç—á—ë—Ç–∞: BUY, SELL, HOLD(0)
        REPORT_NAMES = ['BUY', 'SELL', 'HOLD']

        logger.info("‚öñÔ∏è  –ò—Å–ø–æ–ª—å–∑—É–µ–º –≤–µ—Å–∞ –∏–∑ training_dataset")

        # –î–∞—Ç–∞—Å–µ—Ç—ã LightGBM
        train_data = lgb.Dataset(X_train, label=y_train, weight=w_train)
        val_data = lgb.Dataset(X_val, label=y_val, reference=train_data)

        # ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê
        # SCALER (–û–ü–¶–ò–û–ù–ê–õ–¨–ù–û)
        # ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê
        scaler = None
        if use_scaler:
            logger.info("üìä –°–æ–∑–¥–∞–Ω–∏–µ StandardScaler –∏ –Ω–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏—è –¥–∞–Ω–Ω—ã—Ö...")
            scaler = StandardScaler()
            scaler.fit(X_train)

            # –ü—Ä–∏–º–µ–Ω—è–µ–º –Ω–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏—é
            X_train_scaled = scaler.transform(X_train)
            X_val_scaled = scaler.transform(X_val)

            # –ü–µ—Ä–µ—Å–æ–∑–¥–∞—ë–º –¥–∞—Ç–∞—Å–µ—Ç—ã —Å –Ω–æ—Ä–º–∞–ª–∏–∑–æ–≤–∞–Ω–Ω—ã–º–∏ –¥–∞–Ω–Ω—ã–º–∏
            train_data = lgb.Dataset(X_train_scaled, label=y_train, weight=w_train)
            val_data = lgb.Dataset(X_val_scaled, label=y_val, reference=train_data)

            logger.info(f"‚úÖ Scaler –æ–±—É—á–µ–Ω –∏ –ø—Ä–∏–º–µ–Ω–µ–Ω –Ω–∞ {len(X_train)} –æ–±—Ä–∞–∑—Ü–∞—Ö")
        else:
            logger.info("‚ö†Ô∏è  Scaler –æ—Ç–∫–ª—é—á–µ–Ω - –æ–±—É—á–µ–Ω–∏–µ –Ω–∞ RAW –ø—Ä–∏–∑–Ω–∞–∫–∞—Ö")
            # –î–∞—Ç–∞—Å–µ—Ç—ã —É–∂–µ —Å–æ–∑–¥–∞–Ω—ã –≤—ã—à–µ —Å –∏—Å—Ö–æ–¥–Ω—ã–º–∏ –¥–∞–Ω–Ω—ã–º–∏

        # –ü–∞—Ä–∞–º–µ—Ç—Ä—ã –º–æ–¥–µ–ª–∏ (3 –∫–ª–∞—Å—Å–∞)
        params = {
            'objective': 'multiclass',
            'num_class': NUM_CLASS,
            'metric': 'multi_logloss',
            'boosting_type': 'gbdt',
            'num_leaves': 20,
            'learning_rate': 0.02,
            'feature_fraction': 0.75,
            'bagging_fraction': 0.75,
            'bagging_freq': 5,
            'verbose': -1,
            'min_child_samples': 8,
            'max_depth': 6,
            'lambda_l1': 0.5,
            'lambda_l2': 0.5,
            'min_gain_to_split': 0.05,
            'boost_from_average': False,
            'seed': 42,
            'bagging_seed': 42,
            'feature_fraction_seed': 42,
        }

        # –û–±—É—á–µ–Ω–∏–µ
        model = lgb.train(
            params,
            train_data,
            valid_sets=[val_data],
            valid_names=['valid_0'],
            num_boost_round=1000,
            callbacks=[
                thermometer_progress_callback(logger, width=30, period=10),
                lgb.early_stopping(stopping_rounds=100, first_metric_only=True),
            ],
        )

        # –ü—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è –∏ –º–µ—Ç—Ä–∏–∫–∏ (argmax –ø–æ –≤–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç—è–º)
        if use_scaler and scaler is not None:
            y_val_pred_proba = model.predict(X_val_scaled)
        else:
            y_val_pred_proba = model.predict(X_val)
        y_val_pred = y_val_pred_proba.argmax(axis=1) if getattr(y_val_pred_proba, "ndim", 1) == 2 else y_val_pred_proba

        # ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
        # üîç –î–∏–∞–≥–Ω–æ—Å—Ç–∏–∫–∞ —á–∞—Å—Ç–æ—Ç—ã —Å–∏–≥–Ω–∞–ª–æ–≤ –∏ –ø–æ–¥–±–æ—Ä –ø–æ—Ä–æ–≥–æ–≤
        # ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
        TF2BARS = {"1m": 1440, "3m": 480, "5m": 288, "15m": 96, "30m": 48, "1h": 24}
        tf = str(getattr(self, "timeframe", "5m")).lower()
        bars_per_day = TF2BARS.get(tf, 288)

        # –ü–µ—Ä–µ–±–æ—Ä precision_min 0.55‚Äì0.75 (—à–∞–≥ 0.05)
        precision_grid = [0.45, 0.50, 0.55, 0.60, 0.65, 0.70, 0.75, 0.80, 0.85, 0.9]
        candidates = []
        for pm in precision_grid:
            try:
                tau_i, tstats_i = self.tune_tau_for_spd_range(
                    y_val=np.asarray(y_val),
                    proba=np.asarray(y_val_pred_proba),  # shape (n, 3)
                    bars_per_day=bars_per_day,
                    spd_min=4.0,
                    spd_max=10.0,
                    precision_min=pm,
                    delta=0.08,
                    cooldown_bars=2,
                )
                candidates.append({
                    'precision_min': pm,
                    'tau': float(tau_i),
                    'spd': float(tstats_i.get('spd', float('nan'))),
                    'precision_macro_buy_sell': float(tstats_i.get('precision_macro_buy_sell', float('nan'))),
                    'f1_macro_buy_sell': float(tstats_i.get('f1_macro_buy_sell', float('nan'))),
                    'hit_range': bool(tstats_i.get('hit_range', False)),
                    'delta': float(tstats_i.get('delta', 0.08)),
                    'cooldown_bars': int(tstats_i.get('cooldown_bars', 2)),
                    '_tstats': tstats_i,
                })
            except Exception as e:
                logging.warning(f"precision_min={pm:.2f}: sweep failed with error: {e}")

        def _key(c):
            # 1) hit_range=True; 2) max precision; 3) max f1; 4) min tau
            return (1 if c.get('hit_range') else 0,
                    c.get('precision_macro_buy_sell', float('-inf')),
                    c.get('f1_macro_buy_sell', float('-inf')),
                    -c.get('tau', float('inf')))

        if not candidates:
            raise RuntimeError("Precision sweep failed: no candidates collected")

        best = max(candidates, key=_key)
        tau = best['tau']
        tstats = best['_tstats']
        delta = best.get('delta', 0.08)
        cooldown_bars = best.get('cooldown_bars', 2)

        logging.info("üîß Precision sweep results (pm, tau, spd‚âà, prec‚âà, f1‚âà, hit):")
        for c in candidates:
            logging.info(f"  pm={c['precision_min']:.2f}, tau={c['tau']:.3f}, spd‚âà{c['spd']:.1f}, "
                         f"prec‚âà{c['precision_macro_buy_sell']:.3f}, f1‚âà{c['f1_macro_buy_sell']:.3f}, "
                         f"hit={c['hit_range']}")
        logging.info(f"‚úÖ Picked precision_min={best['precision_min']:.2f} ‚Üí "
                     f"tau={tau:.3f}, spd‚âà{best['spd']:.1f}, "
                     f"precision‚âà{best['precision_macro_buy_sell']:.3f}, f1‚âà{best['f1_macro_buy_sell']:.3f}, "
                     f"hit_range={best['hit_range']}")

        logger.info(
            "üîß Tuned thresholds: tau=%.3f, delta=%.2f, cooldown=%d ‚Üí spd‚âà%.1f/day, "
            "precision‚âà%.3f, recall‚âà%.3f, f1‚âà%.3f (hit_range=%s)"
            % (tau, tstats['delta'], tstats['cooldown_bars'],
               tstats['spd'], tstats['precision_macro_buy_sell'],
               tstats['recall_macro_buy_sell'], tstats['f1_macro_buy_sell'],
               tstats['hit_range'])
        )

        # –ü—Ä–æ—Å—á—ë—Ç SPD –ø–æ –ø—Ä–∏–Ω—è—Ç–æ–π decision policy –¥–ª—è –ª–æ–≥–∞
        if getattr(y_val_pred_proba, "ndim", 1) == 2 and y_val_pred_proba.shape[1] >= 3:
            p_buy = y_val_pred_proba[:, 1]
            p_sell = y_val_pred_proba[:, 2]
            maxp = np.maximum(p_buy, p_sell)
            margin = np.abs(p_buy - p_sell)
            act = (maxp >= tau) & (margin >= delta)

            idx = np.where(act)[0]
            if idx.size > 0:
                keep = [idx[0]]
                for i in idx[1:]:
                    if i - keep[-1] >= cooldown_bars:
                        keep.append(i)
                sel = np.zeros_like(act, dtype=bool)
                sel[np.array(keep, dtype=int)] = True
                act = sel

            spd_est = act.sum() * bars_per_day / max(1, len(y_val))
            logger.info(
                f"üìä Decision policy: tau={tau:.2f}, delta={delta:.2f}, cooldown={cooldown_bars} ‚Üí spd‚âà{spd_est:.1f}/day")

        # —á—É–≤—Å—Ç–≤–∏—Ç–µ–ª—å–Ω–æ—Å—Ç—å –ø–æ tau
        # >>> sensitivity check (–µ–¥–∏–Ω—ã–π –±–ª–æ–∫, —Ä–∞—Å—à–∏—Ä–µ–Ω–Ω–∞—è —Å–µ—Ç–∫–∞ –ø–æ tau, –∫–æ—Ä–æ—Ç–∫–∏–π —Å–≤–æ–¥ –ª–æ–≥–æ–≤)

        # –†–∞—Å—à–∏—Ä—è–µ–º —Å–µ—Ç–∫—É –ø–æ tau: ¬±0.05 –∏ ¬±0.03 –¥–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω–æ –∫ ¬±0.02
        _tau_offsets = [-0.05, -0.03, -0.02, 0.0, 0.02, 0.03, 0.05]
        _delta_offsets = [-0.02, 0.0, 0.02]  # delta –æ—Å—Ç–∞–≤–ª—è–µ–º –∫–∞–∫ –±—ã–ª–æ

        # —á—É–≤—Å—Ç–≤–∏—Ç–µ–ª—å–Ω–æ—Å—Ç—å –ø–æ tau
        tau_sensitivity = []
        for off in _tau_offsets:
            tau_x = float(np.clip(tau + off, 0.0, 1.0))
            r = self._eval_decision_metrics(
                y_true=np.asarray(y_val),
                proba=np.asarray(y_val_pred_proba),
                tau=tau_x,
                delta=delta,
                cooldown_bars=cooldown_bars,
                bars_per_day=bars_per_day,
            )
            tau_sensitivity.append(r)

        # —á—É–≤—Å—Ç–≤–∏—Ç–µ–ª—å–Ω–æ—Å—Ç—å –ø–æ delta
        delta_sensitivity = []
        for off in _delta_offsets:
            delta_x = float(max(0.0, delta + off))
            r = self._eval_decision_metrics(
                y_true=np.asarray(y_val),
                proba=np.asarray(y_val_pred_proba),
                tau=tau,
                delta=delta_x,
                cooldown_bars=cooldown_bars,
                bars_per_day=bars_per_day,
            )
            delta_sensitivity.append(r)

        # ‚îÄ‚îÄ –ö–æ—Ä–æ—Ç–∫–∏–π —Å–≤–æ–¥ –ª–æ–≥–æ–≤: —Ç–µ–∫—É—â–∏–π œÑ –∏ –¥–≤–∞ –±–ª–∏–∂–∞–π—à–∏—Ö —Å–æ—Å–µ–¥–∞ ‚îÄ‚îÄ
        # —Å–æ—Ä—Ç–∏—Ä—É–µ–º –ø–æ –±–ª–∏–∑–æ—Å—Ç–∏ –∫ —Ç–µ–∫—É—â–µ–º—É tau –∏ –±–µ—Ä—ë–º —Ç–æ–ø-3
        _tau_sorted = sorted(tau_sensitivity, key=lambda r: abs(r['tau'] - float(tau)))[:3]
        # —É–ø–æ—Ä—è–¥–æ—á–∏–º –≤ –ª–æ–≥–∞—Ö –ø–æ –≤–æ–∑—Ä–∞—Å—Ç–∞–Ω–∏—é tau
        _tau_sorted = sorted(_tau_sorted, key=lambda r: r['tau'])

        logging.info("üîç Sensitivity (tau near current):")
        for r in _tau_sorted:
            logging.info(f"  tau={r['tau']:.3f} ‚Üí spd‚âà{r['spd']:.1f}, f1‚âà{r['f1_macro_buy_sell']:.3f}")

        # –¥–ª—è delta –ª–æ–≥ –æ—Å—Ç–∞–≤–∏–º –∫–æ–º–ø–∞–∫—Ç–Ω—ã–º (–≤—Å–µ 3 —Ç–æ—á–∫–∏)
        logging.info("üîç Sensitivity (delta¬±0.02):")
        for r in delta_sensitivity:
            logging.info(f"  delta={r['delta']:.2f} ‚Üí spd‚âà{r['spd']:.1f}, f1‚âà{r['f1_macro_buy_sell']:.3f}")

        # –ë–ª–æ–∫ –º–µ—Ç—Ä–∏–∫
        val_acc = accuracy_score(y_val, y_val_pred)
        train_dist = Counter(y_train)
        val_dist = Counter(y_val)
        pred_dist = Counter(y_val_pred)

        logger.info(f"\nüìä –†–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ –∫–ª–∞—Å—Å–æ–≤:")
        logger.info(f"  Train: {dict(train_dist)}")
        logger.info(f"  Val:   {dict(val_dist)}")
        logger.info(f"  Pred:  {dict(pred_dist)}")

        prec, rec, f1, _ = precision_recall_fscore_support(
            y_val, y_val_pred,
            labels=REPORT_LABELS,  # [1,2,0]
            average=None,
            zero_division=0
        )
        cm = confusion_matrix(y_val, y_val_pred, labels=REPORT_LABELS)

        decision_policy = {
            'tau': tau,
            'delta': tstats['delta'],
            'cooldown_bars': tstats['cooldown_bars'],
            'bars_per_day': bars_per_day,
            'val_spd': tstats['spd'],
            'val_precision_macro_buy_sell': tstats['precision_macro_buy_sell'],
            'val_recall_macro_buy_sell': tstats['recall_macro_buy_sell'],
            'val_f1_macro_buy_sell': tstats['f1_macro_buy_sell'],
            'target_spd_range': tstats['range'],
            'hit_range': tstats['hit_range'],
            'precision_min': best.get('precision_min', 0.60),
        }
        precision_min_sweep = [
            {
                'precision_min': c['precision_min'],
                'tau': c['tau'],
                'spd': c['spd'],
                'precision_macro_buy_sell': c['precision_macro_buy_sell'],
                'f1_macro_buy_sell': c['f1_macro_buy_sell'],
                'hit_range': c['hit_range'],
            }
            for c in candidates
        ]

        metrics = {}
        metrics['decision_policy'] = decision_policy
        metrics['precision_min_sweep'] = precision_min_sweep
        metrics.update({
            'val_accuracy': float(val_acc),
            'precision': {name: float(val) for name, val in zip(REPORT_NAMES, prec)},
            'recall': {name: float(val) for name, val in zip(REPORT_NAMES, rec)},
            'f1_score': {name: float(val) for name, val in zip(REPORT_NAMES, f1)},
            'confusion_matrix': cm.tolist(),
            'best_iteration': int(getattr(model, 'best_iteration', 0) or 0),
            'class_distribution': {
                'train': {int(k): int(v) for k, v in train_dist.items()},
                'val': {int(k): int(v) for k, v in val_dist.items()},
                'pred': {int(k): int(v) for k, v in pred_dist.items()}
            },
            'tau_sensitivity': tau_sensitivity,
            'delta_sensitivity': delta_sensitivity,
        })

        # –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –º–æ–¥–µ–ª–∏ –≤ –°–û–í–†–ï–ú–ï–ù–ù–û–ú –§–û–†–ú–ê–¢–ï
        os.makedirs("models", exist_ok=True)
        model_filename = f"models/ml_global_{self.symbol.replace('/', '_')}_{datetime.now().strftime('%Y%m%d_%H%M%S')}.joblib"

        # –ü–æ–¥–≥–æ—Ç–æ–≤–∫–∞ –º–µ—Ç–∞–¥–∞–Ω–Ω—ã—Ö
        model_metadata = {
            'version': '1.1',
            'format': 'modern_dict',
            'instrument': self.symbol,
            'exchange': 'Binance',
            'timeframe': '5m',
            'feature_count': len(self.feature_names),
            'trained_at': datetime.now().isoformat(),
            'training_samples': len(X_train),
            'val_samples': len(X_val),
            'val_accuracy': float(val_acc),
            'best_iteration': int(getattr(model, 'best_iteration', 0) or 0),
            'run_id': run_id,
            'decision_policy': decision_policy,
            'scaler_used': use_scaler,  # –§–ª–∞–≥ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è scaler
        }

        # –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –≤ –ø—Ä–∞–≤–∏–ª—å–Ω–æ–º —Ñ–æ—Ä–º–∞—Ç–µ (—Å–ª–æ–≤–∞—Ä—å)
        model_package = {
            'model': model,
            'scaler': scaler,  # –ú–æ–∂–µ—Ç –±—ã—Ç—å None –µ—Å–ª–∏ use_scaler=False
            'metadata': model_metadata,
            'timeframe': '5m',
            'min_confidence': 0.65,
            'required_warmup': 60
        }

        joblib.dump(model_package, model_filename)
        logger.info(f"‚úÖ –ú–æ–¥–µ–ª—å —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∞ –≤ –°–û–í–†–ï–ú–ï–ù–ù–û–ú –§–û–†–ú–ê–¢–ï: {model_filename}")
        logger.info(f"   - Model: lgb.Booster")
        logger.info(f"   - Scaler: {'StandardScaler' if scaler else 'None (RAW features)'}")
        logger.info(f"   - Metadata: {len(model_metadata)} fields")

        try:
            tau_left = max(0.0, float(tau) - 0.05)
            tau_right = min(0.999, float(tau) + 0.05)
            tau_grid = np.arange(tau_left, tau_right + 1e-9, 0.002)

            spd_curve = []
            f1_curve = []
            for tcur in tau_grid:
                s = self._eval_decision_metrics(
                    y_true=np.asarray(y_val),
                    proba=np.asarray(y_val_pred_proba),
                    tau=float(tcur),
                    delta=float(delta),
                    cooldown_bars=int(cooldown_bars),
                    bars_per_day=int(bars_per_day),
                )
                spd_curve.append(s['spd'])
                f1_curve.append(s['f1_macro_buy_sell'])

            curve_prefix = str(Path("models/training_logs") / Path(model_filename).with_suffix('').name)

            plt.figure(figsize=(7, 4))
            plt.plot(tau_grid, spd_curve, linewidth=2)
            plt.axvline(float(tau), linestyle='--')
            plt.title('SPD vs tau')
            plt.xlabel('tau')
            plt.ylabel('signals per day')
            plt.tight_layout()
            plt.savefig(f"{curve_prefix}_tau_curve_spd.png")
            plt.close()

            plt.figure(figsize=(7, 4))
            plt.plot(tau_grid, f1_curve, linewidth=2)
            plt.axvline(float(tau), linestyle='--')
            plt.title('F1 (macro BUY/SELL on act) vs tau')
            plt.xlabel('tau')
            plt.ylabel('F1 macro (BUY/SELL)')
            plt.tight_layout()
            plt.savefig(f"{curve_prefix}_tau_curve_f1.png")
            plt.close()

        except Exception as _e:
            logging.warning(f"tau curves plotting skipped: {_e}")
        # <<< tau-curves

        # –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –º–µ—Ç—Ä–∏–∫
        self.save_training_report(metrics, model_filename)

        # –î–∏–∞–≥–Ω–æ—Å—Ç–∏–∫–∏
        diag_prefix = Path("models/training_logs") / Path(model_filename).with_suffix('').name
        self.post_training_diagnostics(
            model=model,
            X_val=X_val,
            y_val=y_val,
            y_val_pred_proba=y_val_pred_proba,
            prefix_path=str(diag_prefix),
            bars_per_day=bars_per_day
        )

        return metrics

    def plot_precision_spd_curve(self, y_val, y_val_pred_proba, bars_per_day: int,
                                 delta: float = 0.08, cooldown_bars: int = 2,
                                 prefix_path: str = "models/training_logs/diag") -> None:
        """
        –°—Ç—Ä–æ–∏—Ç –∑–∞–≤–∏—Å–∏–º–æ—Å—Ç–∏ SPD(œÑ) –∏ Precision/Recall/F1 –æ—Ç SPD –¥–ª—è BUY/SELL (macro)
        –∏ —Å–æ—Ö—Ä–∞–Ω—è–µ—Ç:
          - *_tau_sweep.csv
          - *_spd_vs_tau.png
          - *_prf_vs_spd.png
        –ü–∞—Ä–∞–º–µ—Ç—Ä—ã:
          y_val                : np.ndarray[int] (0=HOLD, 1=BUY, 2=SELL)
          y_val_pred_proba     : np.ndarray[float] shape=(n,3)
          bars_per_day         : 288 –¥–ª—è 5m, 96 –¥–ª—è 15m –∏ —Ç.–ø.
          delta                : |p_buy - p_sell| –º–∏–Ω–∏–º—É–º, —á—Ç–æ–±—ã –Ω–µ –±—Ä–∞—Ç—å ¬´—Å–æ–º–Ω–∏—Ç–µ–ª—å–Ω—ã–µ¬ª —Ç–æ—á–∫–∏
          cooldown_bars        : –º–∏–Ω–∏–º–∞–ª—å–Ω—ã–π –∑–∞–∑–æ—Ä –º–µ–∂–¥—É —Å–∏–≥–Ω–∞–ª–∞–º–∏ –≤ –±–∞—Ä–∞—Ö
          prefix_path          : –ø—É—Ç—å –ø—Ä–µ—Ñ–∏–∫—Å–∞ –¥–ª—è —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è –∞—Ä—Ç–µ—Ñ–∞–∫—Ç–æ–≤
        """
        import numpy as np
        import pandas as pd
        import matplotlib.pyplot as plt
        from sklearn.metrics import precision_score, recall_score, f1_score

        proba = np.asarray(y_val_pred_proba)
        p_buy, p_sell = proba[:, 1], proba[:, 2]
        maxp = np.maximum(p_buy, p_sell)
        margin = np.abs(p_buy - p_sell)

        def _apply_cooldown(mask: np.ndarray, cd: int) -> np.ndarray:
            if cd <= 0:
                return mask
            idx = np.where(mask)[0]
            if idx.size == 0:
                return mask
            keep = [idx[0]]
            last = idx[0]
            for i in idx[1:]:
                if i - last >= cd:
                    keep.append(i)
                    last = i
            out = np.zeros_like(mask, dtype=bool)
            out[np.array(keep, dtype=int)] = True
            return out

        taus = np.linspace(0.45, 0.70, 26)  # –º–æ–∂–Ω–æ —Å—É–∑–∏—Ç—å/—Ä–∞—Å—à–∏—Ä–∏—Ç—å
        rows = []
        n = len(y_val)

        for tau in taus:
            act = (maxp >= tau) & (margin >= delta)
            act = _apply_cooldown(act, cooldown_bars)

            signals = int(act.sum())
            spd = signals * bars_per_day / max(1, n)

            if signals == 0:
                prec = rec = f1 = 0.0
            else:
                pred_dir = np.where(p_buy[act] >= p_sell[act], 1, 2)  # 1=BUY, 2=SELL
                true_dir = y_val[act]
                # macro –ø–æ BUY/SELL (–±–µ–∑ HOLD)
                prec = precision_score(true_dir, pred_dir, labels=[1, 2], average='macro', zero_division=0)
                rec = recall_score(true_dir, pred_dir, labels=[1, 2], average='macro', zero_division=0)
                f1 = f1_score(true_dir, pred_dir, labels=[1, 2], average='macro', zero_division=0)

            rows.append((tau, spd, prec, rec, f1, signals))

        df = pd.DataFrame(rows, columns=['tau', 'spd_per_day', 'precision', 'recall', 'f1', 'signals'])
        csv_path = f"{prefix_path}_tau_sweep.csv"
        df.to_csv(csv_path, index=False)

        # ‚îÄ‚îÄ –ì—Ä–∞—Ñ–∏–∫ SPD(œÑ)
        plt.figure(figsize=(8, 5))
        plt.plot(df['tau'], df['spd_per_day'], marker='o')
        plt.xlabel('tau')
        plt.ylabel('SPD (signals/day)')
        plt.title('Signals per day vs tau')
        plt.grid(True, alpha=0.3)
        plt.tight_layout()
        plt.savefig(f"{prefix_path}_spd_vs_tau.png")
        plt.close()

        # ‚îÄ‚îÄ –ì—Ä–∞—Ñ–∏–∫ Precision/Recall/F1 vs SPD
        plt.figure(figsize=(8, 5))
        plt.plot(df['spd_per_day'], df['precision'], marker='o', label='Precision (macro BUY/SELL)')
        plt.plot(df['spd_per_day'], df['recall'], marker='o', label='Recall (macro BUY/SELL)')
        plt.plot(df['spd_per_day'], df['f1'], marker='o', label='F1 (macro BUY/SELL)')
        plt.xlabel('SPD (signals/day)')
        plt.ylabel('score')
        plt.title('Precision / Recall / F1 vs SPD')
        plt.legend()
        plt.grid(True, alpha=0.3)
        plt.tight_layout()
        plt.savefig(f"{prefix_path}_prf_vs_spd.png")
        plt.close()

    def post_training_diagnostics(self, model, X_val, y_val, y_val_pred_proba,
                                  prefix_path: str, bars_per_day: int = 288):
        """
        –î–∏–∞–≥–Ω–æ—Å—Ç–∏–∫–∏ –ø–æ—Å–ª–µ –æ–±—É—á–µ–Ω–∏—è...
        """
        os.makedirs(os.path.dirname(prefix_path), exist_ok=True)

        # –≥–∞—Ä–∞–Ω—Ç–∏—Ä—É–µ–º –Ω–µ-None –∏ int
        try:
            bars_per_day = int(bars_per_day) if bars_per_day is not None else 288
        except Exception:
            bars_per_day = 288
        """
        –î–∏–∞–≥–Ω–æ—Å—Ç–∏–∫–∏ –ø–æ—Å–ª–µ –æ–±—É—á–µ–Ω–∏—è:
        - –≤–∞–∂–Ω–æ—Å—Ç—å –ø—Ä–∏–∑–Ω–∞–∫–æ–≤ (gain)
        - –≥–∏—Å—Ç–æ–≥—Ä–∞–º–º—ã –≤–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç–µ–π p(BUY)/p(SELL) –¥–ª—è TP/FP/FN/TN
        - max-proba vs –∏—Å—Ç–∏–Ω–Ω—ã–π/–ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–Ω—ã–π –∫–ª–∞—Å—Å
        - PR-–∫—Ä–∏–≤—ã–µ (one-vs-rest) –¥–ª—è 3 –∫–ª–∞—Å—Å–æ–≤
        –í—Å–µ —Å–æ—Ö—Ä–∞–Ω—è–µ—Ç—Å—è —Å –ø—Ä–µ—Ñ–∏–∫—Å–æ–º prefix_path_* –≤ models/training_logs.
        """

        os.makedirs(os.path.dirname(prefix_path), exist_ok=True)
        # === 0) –ü–æ–¥–≥–æ—Ç–æ–≤–∫–∞ ===
        # –æ–∂–∏–¥–∞–µ–º –º–µ—Ç–∫–∏: 0=HOLD, 1=BUY, 2=SELL  (—É —Ç–µ–±—è —Ç–∞–∫ –≤ —Ç—Ä–µ–Ω–µ—Ä–µ)
        class_names = ['BUY', 'SELL', 'HOLD']  # –¥–ª—è –≤—ã–≤–æ–¥–∞ –≤ –ø–æ—Ä—è–¥–∫–µ [1,2,0]
        label_order = [1, 2, 0]

        proba = np.asarray(y_val_pred_proba)
        if proba.ndim == 1:
            # –µ—Å–ª–∏ –≤–¥—Ä—É–≥ —É–∂–µ –º–µ—Ç–∫–∏, –∞ –Ω–µ proba ‚Äî –±—ã—Å—Ç—Ä–æ ¬´—Ä–∞—Å—â–µ–ø–∏—Ç—å¬ª –≤ —Ñ–∏–∫—Ç–∏–≤–Ω—ã–π one-hot
            k = int(proba.max()) + 1
            tmp = np.zeros((len(proba), 3), dtype=float)
            tmp[np.arange(len(proba)), np.clip(proba.astype(int), 0, 2)] = 1.0
            proba = tmp

        # –∏–Ω–¥–µ–∫—Å—ã –∫–æ–ª–æ–Ω–æ–∫ –≤ proba –ø–æ –º–µ—Ç–∫–∞–º [0,1,2]
        p_hold = proba[:, 0]
        p_buy = proba[:, 1]
        p_sell = proba[:, 2]

        # –µ—Å–ª–∏ –µ—Å—Ç—å feature_names ‚Äî –∏—Å–ø–æ–ª—å–∑—É–µ–º –¥–ª—è –∏–º–ø–æ—Ä—Ç–∞
        feat_names = getattr(self, "feature_names", None)
        if feat_names is None or len(feat_names) != X_val.shape[1]:
            feat_names = [f"f{i}" for i in range(X_val.shape[1])]

        # === 1) –í–∞–∂–Ω–æ—Å—Ç—å –ø—Ä–∏–∑–Ω–∞–∫–æ–≤ (gain) ===
        try:
            gain = model.feature_importance(importance_type='gain')
            df_imp = (pd.DataFrame({'feature': feat_names, 'gain': gain})
                      .sort_values('gain', ascending=False)
                      .head(30))
            plt.figure(figsize=(9.0, max(4.0, 0.3 * float(len(df_imp)))))
            sns.barplot(data=df_imp, x='gain', y='feature')
            plt.title('Feature Importance (gain) ‚Äî top 30')
            plt.tight_layout()
            plt.savefig(f"{prefix_path}_feat_importance.png")
            plt.close()
        except Exception as e:
            try:
                split = model.feature_importance(importance_type='split')
                df_imp = (pd.DataFrame({'feature': feat_names, 'split': split})
                          .sort_values('split', ascending=False)
                          .head(30))
                plt.figure(figsize=(9.0, max(4.0, 0.3 * float(len(df_imp)))))
                sns.barplot(data=df_imp, x='split', y='feature')
                plt.title('Feature Importance (split) ‚Äî top 30')
                plt.tight_layout()
                plt.savefig(f"{prefix_path}_feat_importance.png")
                plt.close()
            except Exception:
                pass  # –Ω–µ—Ç –≤–∞–∂–Ω–æ—Å—Ç–µ–π ‚Äî –ø—Ä–æ–ø—É—Å–∫–∞–µ–º

        # === 2) –ì–∏—Å—Ç–æ–≥—Ä–∞–º–º—ã p(BUY)/p(SELL) –¥–ª—è TP/FP/FN/TN ===
        # —Å–æ–±–µ—Ä–µ–º –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–Ω—ã–µ –º–µ—Ç–∫–∏
        y_pred = proba.argmax(axis=1)

        def hist_one(prob, true_class, name, fname):
            mask_pos = (y_val == true_class)
            mask_pred_pos = (y_pred == true_class)

            # TP: true=class & pred=class
            tp = prob[mask_pos & mask_pred_pos]
            # FP: true!=class & pred=class
            fp = prob[(~mask_pos) & mask_pred_pos]
            # FN: true=class & pred!=class
            fn = prob[mask_pos & (~mask_pred_pos)]
            # TN: true!=class & pred!=class
            tn = prob[(~mask_pos) & (~mask_pred_pos)]

            plt.figure(figsize=(8, 5))
            bins = 30
            sns.histplot(tp, bins=bins, stat='density', label='TP', alpha=0.6)
            sns.histplot(fp, bins=bins, stat='density', label='FP', alpha=0.6)
            sns.histplot(fn, bins=bins, stat='density', label='FN', alpha=0.6)
            sns.histplot(tn, bins=bins, stat='density', label='TN', alpha=0.6)
            plt.legend()
            plt.xlabel(f"p({name})")
            plt.ylabel("density")
            plt.title(f"Distributions for {name}")
            plt.tight_layout()
            plt.savefig(fname)
            plt.close()

        hist_one(p_buy, 1, "BUY", f"{prefix_path}_proba_hist_BUY.png")
        hist_one(p_sell, 2, "SELL", f"{prefix_path}_proba_hist_SELL.png")

        # === 3) Max-proba vs –∫–ª–∞—Å—Å—ã ===
        maxp = proba.max(axis=1)
        plt.figure(figsize=(8, 5))
        sns.scatterplot(x=np.arange(len(maxp)), y=maxp,
                        hue=[{0: 'HOLD', 1: 'BUY', 2: 'SELL'}[c] for c in y_val],
                        s=12, linewidth=0)
        plt.title("Max class probability vs true class (val order)")
        plt.xlabel("index in validation set (chronological)")
        plt.ylabel("max proba")
        plt.tight_layout()
        plt.savefig(f"{prefix_path}_maxproba_scatter.png")
        plt.close()

        # === 4) PR-–∫—Ä–∏–≤—ã–µ (one-vs-rest) ===
        Y_bin = label_binarize(y_val, classes=[0, 1, 2])  # —Å—Ç–æ–ª–±—Ü—ã: HOLD, BUY, SELL
        curves = [
            ("BUY", Y_bin[:, 1], p_buy),
            ("SELL", Y_bin[:, 2], p_sell),
            ("HOLD", Y_bin[:, 0], p_hold),
        ]
        plt.figure(figsize=(8, 6))
        for name, y_true_bin, y_score in curves:
            precision, recall, _ = precision_recall_curve(y_true_bin, y_score)
            ap = average_precision_score(y_true_bin, y_score)
            plt.plot(recall, precision, label=f"{name} (AP={ap:.3f})")
        plt.xlabel("Recall")
        plt.ylabel("Precision")
        plt.title("Precision‚ÄìRecall curves (one-vs-rest)")
        plt.legend()
        plt.tight_layout()
        plt.savefig(f"{prefix_path}_pr_curves.png")
        plt.close()

        self.plot_precision_spd_curve(
            y_val=y_val,
            y_val_pred_proba=y_val_pred_proba,
            bars_per_day=bars_per_day,  # 288 –¥–ª—è 5m; –ø–µ—Ä–µ–¥–∞–π –∏–∑ train_model
            delta=0.08,
            cooldown_bars=2,
            prefix_path=prefix_path
        )

    def save_training_report(self, metrics: dict, model_path: str):
        """–°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –æ—Ç—á–µ—Ç–∞ –æ –æ–±—É—á–µ–Ω–∏–∏"""
        os.makedirs("models/training_logs", exist_ok=True)

        report = {
            'training_date': datetime.now().isoformat(),
            'symbol': self.symbol,
            'db_dsn': self.db_dsn,
            'model_path': model_path,
            'metrics': metrics,
            'feature_names': self.feature_names
        }

        report_filename = f"models/training_logs/training_report_{datetime.now().strftime('%Y%m%d_%H%M%S')}.json"
        with open(report_filename, 'w') as f:
            json.dump(report, f, indent=2)

        # –í–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏—è confusion matrix —Å –∫–æ—Ä—Ä–µ–∫—Ç–Ω—ã–º–∏ –ø–æ–¥–ø–∏—Å—è–º–∏
        try:
            import numpy as np
            import matplotlib.pyplot as plt
            import seaborn as sns

            cm = np.array(metrics.get('confusion_matrix', []))
            if cm.size == 0:
                raise ValueError("confusion_matrix –ø—É—Å—Ç–∞—è –∏–ª–∏ –æ—Ç—Å—É—Ç—Å—Ç–≤—É–µ—Ç –≤ metrics")

            n = cm.shape[0]

            # –ü–æ–ø—ã—Ç–∫–∞ –≤–∑—è—Ç—å –ø–æ–¥–ø–∏—Å–∏ –∏–∑ metrics['precision'].keys()
            prec_map = metrics.get('precision', {})
            labels = list(prec_map.keys()) if isinstance(prec_map, dict) else None

            # –§–æ–ª–±—ç–∫–∏ –ø–æ —Ä–∞–∑–º–µ—Ä—É –º–∞—Ç—Ä–∏—Ü—ã
            if not labels or len(labels) != n:
                if n == 3:
                    # –ü–æ—Ä—è–¥–æ–∫ —Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤—É–µ—Ç –æ—Ç—á–µ—Ç—É: BUY, SELL, HOLD
                    labels = ['BUY', 'SELL', 'HOLD']
                elif n == 4:
                    # –ï—Å–ª–∏ –≥–¥–µ-—Ç–æ –æ—Å—Ç–∞–ª–∏—Å—å 4 –∫–ª–∞—Å—Å–∞
                    labels = ['BUY', 'SELL', 'HOLD', 'ANTI']
                else:
                    labels = [f"class_{i}" for i in range(n)]

            plt.figure(figsize=(8, 6))
            sns.heatmap(cm, annot=True, fmt='d',
                        xticklabels=labels, yticklabels=labels)
            plt.title('Confusion Matrix')
            plt.tight_layout()
            plt.savefig(report_filename.replace('.json', '_cm.png'))
            plt.close()
        except Exception as e:
            logger.warning(f"–ù–µ —É–¥–∞–ª–æ—Å—å —Å–æ–∑–¥–∞—Ç—å –≤–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏—é: {e}")

        logger.info(f"‚úÖ –û—Ç—á–µ—Ç —Å–æ—Ö—Ä–∞–Ω–µ–Ω: {report_filename}")

    def close(self):
        """–ö–æ—Ä—Ä–µ–∫—Ç–Ω–æ–µ –∑–∞–∫—Ä—ã—Ç–∏–µ"""
        self.data_loader.close()


# ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
# MAIN
# ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ

def main():
    """–û—Å–Ω–æ–≤–Ω–∞—è —Ñ—É–Ω–∫—Ü–∏—è"""
    print("üöÄ –ó–ê–ü–£–°–ö –û–ë–£–ß–ï–ù–ò–Ø –ú–û–î–ï–õ–ò (v2.0 - SQLite)")
    print("=" * 50)

    # –ü—Ä–æ–≤–µ—Ä–∫–∞ —Å—É—â–µ—Å—Ç–≤–æ–≤–∞–Ω–∏—è –ë–î
    db_file = DATA_DIR / "market_data.sqlite"
    if not db_file.exists():
        print(f"‚ùå –ë–∞–∑–∞ –¥–∞–Ω–Ω—ã—Ö {db_file} –Ω–µ –Ω–∞–π–¥–µ–Ω–∞!")
        print("   –°–Ω–∞—á–∞–ª–∞ –∑–∞–ø—É—Å—Ç–∏—Ç–µ:")
        print("   1. ml_data_preparation.py - –¥–ª—è –∑–∞–≥—Ä—É–∑–∫–∏ –¥–∞–Ω–Ω—ã—Ö")
        print("   2. ml_labeling_tool_v3.py - –¥–ª—è —Ä–∞–∑–º–µ—Ç–∫–∏ –¥–∞–Ω–Ω—ã—Ö")
        return 1
    trainer = None
    try:
        # –ù–∞—Å—Ç—Ä–æ–π–∫–∏
        db_dsn = MARKET_DB_DSN
        symbol = "ETHUSDT"
        # –ü–æ–ª—É—á–∏—Ç—å –ø–æ—Å–ª–µ–¥–Ω–∏–π run_id
        engine = create_engine(MARKET_DB_DSN)
        with engine.connect() as conn:
            result = conn.execute(text("SELECT run_id FROM training_dataset_meta ORDER BY created_at DESC LIMIT 1"))
            row = result.fetchone()
        engine.dispose()

        if not row:
            print("‚ùå –ù–µ—Ç –≥–æ—Ç–æ–≤—ã—Ö snapshot –≤ training_dataset_meta!")
            print("   –°–æ–∑–¥–∞–π—Ç–µ snapshot —á–µ—Ä–µ–∑ [14] –≤ ml_labeling_tool_v3.py")
            return 1

        run_id = row[0]

        print(f"üìä –ë–∞–∑–∞ –¥–∞–Ω–Ω—ã—Ö: {DATA_DIR / 'market_data.sqlite'}")
        print(f"üéØ –°–∏–º–≤–æ–ª: {symbol}")
        print(f"üì¶ Run ID: {run_id}")
        print("=" * 50)

        # –û–±—É—á–µ–Ω–∏–µ
        trainer = ModelTrainer(db_dsn, symbol)

        # –ü–∞—Ä–∞–º–µ—Ç—Ä use_scaler –º–æ–∂–Ω–æ –Ω–∞—Å—Ç—Ä–æ–∏—Ç—å (–ø–æ —É–º–æ–ª—á–∞–Ω–∏—é True)
        use_scaler = False  # –£—Å—Ç–∞–Ω–æ–≤–∏—Ç—å False –¥–ª—è –æ–±—É—á–µ–Ω–∏—è –Ω–∞ RAW –ø—Ä–∏–∑–Ω–∞–∫–∞—Ö
        metrics = trainer.train_model(run_id, use_scaler=use_scaler)

        # –í—ã–≤–æ–¥ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤
        print("\nüéØ –†–ï–ó–£–õ–¨–¢–ê–¢–´ –û–ë–£–ß–ï–ù–ò–Ø:")
        print(f"   –¢–æ—á–Ω–æ—Å—Ç—å: {metrics['val_accuracy']:.4f}")
        print(
            f"   Precision BUY/SELL/HOLD: {metrics['precision']['BUY']:.4f}/{metrics['precision']['SELL']:.4f}/{metrics['precision']['HOLD']:.4f}")
        print(
            f"   Recall BUY/SELL/HOLD: {metrics['recall']['BUY']:.4f}/{metrics['recall']['SELL']:.4f}/{metrics['recall']['HOLD']:.4f}")
        print(
            f"   F1 BUY/SELL/HOLD: {metrics['f1_score']['BUY']:.4f}/{metrics['f1_score']['SELL']:.4f}/{metrics['f1_score']['HOLD']:.4f}")
        print("\n‚úÖ –û–±—É—á–µ–Ω–∏–µ –∑–∞–≤–µ—Ä—à–µ–Ω–æ —É—Å–ø–µ—à–Ω–æ!")
        return 0

    except Exception as e:
        print(f"‚ùå –û—à–∏–±–∫–∞: {e}")
        return 1
    finally:
        if trainer:
            trainer.close()


if __name__ == '__main__':
    sys.exit(main())