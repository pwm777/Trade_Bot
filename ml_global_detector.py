"""
ml_global_detector.py
ML-–¥–µ—Ç–µ–∫—Ç–æ—Ä —Ä–∞–∑–≤–æ—Ä–æ—Ç–æ–≤ –Ω–∞ –æ—Å–Ω–æ–≤–µ LightGBM –¥–ª—è –≥–ª–æ–±–∞–ª—å–Ω–æ–≥–æ —Ç–∞–π–º—Ñ—Ä–µ–π–º–∞ (5m)
–û–ø–∏—Å–∞–Ω–∏–µ:
- –ò—Å–ø–æ–ª—å–∑—É–µ—Ç  –ø—Ä–∏–∑–Ω–∞–∫–∏ –∏–∑ OHLCV –¥–∞–Ω–Ω—ã—Ö ETH/USDT
- –ö–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏—è: FLAT (0), BUY reversal (1), SELL reversal (2)
- –†–∞–±–æ—Ç–∞–µ—Ç —Å–æ–≤–º–µ—Å—Ç–Ω–æ —Å cosum-–¥–µ—Ç–µ–∫—Ç–æ—Ä–∞–º–∏ (1m)
- –ê–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏–π fallback –Ω–∞ cosum –ø—Ä–∏ –æ—Ç—Å—É—Ç—Å—Ç–≤–∏–∏ –º–æ–¥–µ–ª–∏

"""

from typing import Dict, Optional, Any
import numpy as np
import pandas as pd
from datetime import datetime
import os
import logging
from datetime import UTC
import lightgbm as lgb
import joblib

from iqts_standards import (
    DetectorSignal, Detector,
     normalize_signal, Timeframe)

class MLGlobalDetector(Detector):
    """
    ML-–¥–µ—Ç–µ–∫—Ç–æ—Ä –Ω–∞ –æ—Å–Ω–æ–≤–µ LightGBM –¥–ª—è –≥–ª–æ–±–∞–ª—å–Ω–æ–≥–æ —Ç–∞–π–º—Ñ—Ä–µ–π–º–∞ (5m)

    –ü—Ä–∏–Ω—Ü–∏–ø —Ä–∞–±–æ—Ç—ã:
    1. –ü–æ–ª—É—á–∞–µ—Ç OHLCV –¥–∞–Ω–Ω—ã–µ 5m –∏–∑ market_data
    3. –ù–æ—Ä–º–∞–ª–∏–∑—É–µ—Ç –ø—Ä–∏–∑–Ω–∞–∫–∏ —á–µ—Ä–µ–∑ StandardScaler
    4. –ü—Ä–µ–¥—Å–∫–∞–∑—ã–≤–∞–µ—Ç –∫–ª–∞—Å—Å —á–µ—Ä–µ–∑ –æ–±—É—á–µ–Ω–Ω—É—é LightGBM –º–æ–¥–µ–ª—å
    5. –í–æ–∑–≤—Ä–∞—â–∞–µ—Ç DetectorSignal –≤ —Å—Ç–∞–Ω–¥–∞—Ä—Ç–Ω–æ–º —Ñ–æ—Ä–º–∞—Ç–µ IQTS

    –ö–ª–∞—Å—Å—ã:
    - 0: FLAT (–Ω–µ—Ç —Ä–∞–∑–≤–æ—Ä–æ—Ç–∞)
    - 1: BUY reversal (—Ä–∞–∑–≤–æ—Ä–æ—Ç –≤–≤–µ—Ä—Ö)
    - 2: SELL reversal (—Ä–∞–∑–≤–æ—Ä–æ—Ç –≤–Ω–∏–∑)

    –ò–Ω—Ç–µ–≥—Ä–∞—Ü–∏—è:
    - –†–µ–∞–ª–∏–∑—É–µ—Ç –∏–Ω—Ç–µ—Ä—Ñ–µ–π—Å DetectorInterface
    - –ó–∞–º–µ–Ω—è–µ—Ç GlobalTrendDetector –≤ ExitSignalDetector
    - –†–∞–±–æ—Ç–∞–µ—Ç —Å –∫–∞—Å–∫–∞–¥–Ω–æ–π –ª–æ–≥–∏–∫–æ–π exit_system.py

    –ü—Ä–∏–º–µ—Ä –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è:
        detector = MLGlobalDetector(
            timeframe='5m',
            model_path='models/ml_global_5m_lgbm.joblib',
            name='ml_global_5m'
        )
        # signal = {'ok': True, 'direction': 'BUY', 'confidence': 0.78, ...}
    """

    def __init__(self, timeframe: Timeframe = "5m",
                 model_path: str = 'models/ml_global_5m_lgbm.joblib',
                 use_fallback: bool = False,
                 name: str = None, use_scaler: Optional[bool] = None):

        super().__init__(name or f"ml_global_{timeframe}")

        abs_path = os.path.abspath(model_path)
        self.logger.setLevel(logging.INFO)
        self.last_confidence = None
        self.timeframe = timeframe
        self.use_fallback = use_fallback
        self.model_path = model_path

        # –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –æ—Å–Ω–æ–≤–Ω—ã—Ö –∞—Ç—Ä–∏–±—É—Ç–æ–≤ –º–æ–¥–µ–ª–∏
        self.model: Optional[lgb.Booster] = None
        self.use_scaler = use_scaler
        self.feature_names = [
            'cmo_14', 'volume', 'trend_acceleration_ema7', 'regime_volatility',
            'bb_width', 'adx_14', 'plus_di_14', 'minus_di_14', 'atr_14_normalized',
            'volume_ratio_ema3', 'candle_relative_body', 'upper_shadow_ratio',
            'lower_shadow_ratio', 'price_vs_vwap', 'bb_position', 'cusum_1m_recent',
            'cusum_1m_quality_score', 'cusum_1m_trend_aligned', 'cusum_1m_price_move',
            'is_trend_pattern_1m', 'body_to_range_ratio_1m', 'close_position_in_range_1m'
        ]

        self.min_confidence = 0.53
        self.scaler = None
        self.required_warmup = 60

        # –ú–µ—Ç–∞–¥–∞–Ω–Ω—ã–µ –º–æ–¥–µ–ª–∏
        self.model_metadata = {
            'version': '1.0',
            'instrument': 'ETH/USDT',
            'exchange': 'Binance',
            'timeframe': timeframe,
            'feature_count': len(self.feature_names),
            'trained_at': None,
            'training_samples': None,
            'val_accuracy': None
        }

        # ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê
        # –ü–†–û–°–¢–ê–Ø –ó–ê–ì–†–£–ó–ö–ê –ú–û–î–ï–õ–ò –ë–ï–ó –†–ï–ö–£–†–°–ò–ò
        # ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê
        if model_path and os.path.exists(abs_path):
            try:
                self.load_model(abs_path)
                self.logger.info(f"‚úÖ ML –º–æ–¥–µ–ª—å —É—Å–ø–µ—à–Ω–æ –∑–∞–≥—Ä—É–∂–µ–Ω–∞ –∏–∑ {abs_path}")
            except Exception as e:
                self.logger.error(f"‚ùå –û—à–∏–±–∫–∞ –∑–∞–≥—Ä—É–∑–∫–∏ –º–æ–¥–µ–ª–∏: {e}")
                if not use_fallback:
                    raise
                else:
                    self.logger.warning("üîÑ –†–µ–∂–∏–º fallback –∞–∫—Ç–∏–≤–∏—Ä–æ–≤–∞–Ω")
        else:
            self.logger.error(f"‚ùå –§–∞–π–ª –º–æ–¥–µ–ª–∏ –Ω–µ –Ω–∞–π–¥–µ–Ω: {abs_path}")
            if not use_fallback:
                raise FileNotFoundError(f"Model file not found: {abs_path}")
            else:
                self.logger.warning("üîÑ –†–µ–∂–∏–º fallback –∞–∫—Ç–∏–≤–∏—Ä–æ–≤–∞–Ω")

    def get_status(self) -> Dict[str, Any]:
        """–ü–æ–ª—É—á–µ–Ω–∏–µ —Å—Ç–∞—Ç—É—Å–∞ ML –¥–µ—Ç–µ–∫—Ç–æ—Ä–∞"""
        return {
            'ok': self.model is not None,
            'confidence': self.last_confidence,
            'model_loaded': self.model is not None,
            'scaler_available': getattr(self, 'scaler', None) is not None,
            'feature_count': len(self.feature_names),
            'required_warmup': self.required_warmup,
            'min_confidence': self.min_confidence,
            'model_metadata': self.model_metadata
        }

    # ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê
    # –ò–ó–í–õ–ï–ß–ï–ù–ò–ï –ü–†–ò–ó–ù–ê–ö–û–í
    # ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê

    def extract_features(self, df: pd.DataFrame) -> np.ndarray:
        """
        –ò–∑–≤–ª–µ–∫–∞–µ—Ç –≤—Å–µ –ø—Ä–∏–∑–Ω–∞–∫–∏ —Å –¥–µ—Ç–∞–ª—å–Ω–æ–π –¥–∏–∞–≥–Ω–æ—Å—Ç–∏–∫–æ–π
        """

        # üî• –î–ï–¢–ê–õ–¨–ù–ê–Ø –î–ò–ê–ì–ù–û–°–¢–ò–ö–ê –§–ò–ß
        # –ü—Ä–æ–≤–µ—Ä–∫–∞ –Ω–∞–ª–∏—á–∏—è –≤—Å–µ—Ö feature –∫–æ–ª–æ–Ω–æ–∫
        missing_features = [col for col in self.feature_names if col not in df.columns]
        available_features = [col for col in self.feature_names if col in df.columns]

        if missing_features:
            self.logger.error(f"‚ùå MISSING FEATURES ({len(missing_features)}): {missing_features}")
            self.logger.info(f"‚úÖ AVAILABLE FEATURES ({len(available_features)}): {available_features}")

            # –ü—Ä–æ–≤–µ—Ä—è–µ–º –ø–µ—Ä–≤—ã–µ –Ω–µ—Å–∫–æ–ª—å–∫–æ —Å—Ç—Ä–æ–∫ –¥–æ—Å—Ç—É–ø–Ω—ã—Ö —Ñ–∏—á
            for feature in available_features[:5]:  # –ø–µ—Ä–≤—ã–µ 5 —á—Ç–æ–±—ã –Ω–µ –∑–∞—Å–ø–∞–º–∏—Ç—å
                sample_value = df[feature].iloc[-1] if len(df) > 0 else "N/A"
                self.logger.info(f"   {feature}: {sample_value}")

            raise ValueError(f"Missing ML features: {missing_features}")

        # –ü—Ä–æ–≤–µ—Ä–∫–∞ –∫–∞—á–µ—Å—Ç–≤–∞ –¥–∞–Ω–Ω—ã—Ö –≤ —Ñ–∏—á–∞—Ö
        problematic_features = []
        for feature_name in self.feature_names:
            values = df[feature_name]
            if values.isna().all() or values.isnull().all():
                problematic_features.append(f"{feature_name} (all NaN)")
            elif (values == 0).all():
                problematic_features.append(f"{feature_name} (all zeros)")

        if problematic_features:
            self.logger.warning(f"‚ö†Ô∏è PROBLEMATIC FEATURES: {problematic_features}")

        # –ò–∑–≤–ª–µ–∫–∞–µ–º –ø–æ—Å–ª–µ–¥–Ω–∏–µ –∑–Ω–∞—á–µ–Ω–∏—è –≤—Å–µ—Ö –ø—Ä–∏–∑–Ω–∞–∫–æ–≤
        features = []
        for feature_name in self.feature_names:
            value = df[feature_name].iloc[-1]

            # –û–±—Ä–∞–±–æ—Ç–∫–∞ None/NaN
            if pd.isna(value):
                self.logger.warning(f"Feature '{feature_name}' is NaN, replacing with 0.0")
                value = 0.0

            features.append(float(value))

        # –ü—Ä–µ–æ–±—Ä–∞–∑—É–µ–º –≤ numpy array (1, n_features)
        features_array = np.array(features).reshape(1, -1)

        # –í–∞–ª–∏–¥–∞—Ü–∏—è –Ω–∞ NaN/Inf
        if not self._validate_features(features_array):
            self.logger.warning("Features contain NaN/Inf, cleaning...")
            features_array = np.nan_to_num(features_array, nan=0.0, posinf=0.0, neginf=0.0)

        self.logger.info("‚úÖ ML FEATURE DIAGNOSTIC - OK")
        return features_array

    # ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê
    # –û–°–ù–û–í–ù–û–ô –ú–ï–¢–û–î –ê–ù–ê–õ–ò–ó–ê
    # ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê

    async def analyze(self, data: Dict[Timeframe, pd.DataFrame]) -> DetectorSignal:
        """
        –ò–Ω—Ñ–µ—Ä–µ–Ω—Å LightGBM –ø–æ –≤—Ö–æ–¥–Ω—ã–º –¥–∞–Ω–Ω—ã–º –¥–ª—è –∑–∞–¥–∞–Ω–Ω–æ–≥–æ —Ç–∞–π–º—Ñ—Ä–µ–π–º–∞.
        """
        self.logger.info(f"üîÑ –ê–Ω–∞–ª–∏–∑ —Ç—Ä–µ–Ω–¥–∞ –¥–µ—Ç–µ–∫—Ç–æ—Ä–æ–º LightGBM ")
        # 1) –í–∞–ª–∏–¥–∞—Ü–∏—è —Å—Ç—Ä—É–∫—Ç—É—Ä—ã –≤—Ö–æ–¥–∞ - –£–ü–†–û–©–ï–ù–ù–ê–Ø –í–ê–õ–ò–î–ê–¶–ò–Ø
        if not data or not isinstance(data, dict):
            self.logger.error(f"‚ùå Invalid data structure: {type(data)}")
            return normalize_signal({
                "ok": False,
                "direction": 0, #FLAT
                "confidence": 0.0,
                "reason": "invalid_data_structure",
                "metadata": {"detector": "ml", "timeframe": self.timeframe}
            })

        # 2) –ù–∞–ª–∏—á–∏–µ –Ω—É–∂–Ω–æ–≥–æ –¢–§
        if self.timeframe not in data:
            self.logger.error(f"‚ùå Missing timeframe {self.timeframe} in data. Available: {list(data.keys())}")
            return normalize_signal({
                "ok": False,
                "direction": 0, #"FLAT"
                "confidence": 0.0,
                "reason": "missing_timeframe",
                "metadata": {"detector": "ml", "missing_tf": self.timeframe, "available_tfs": list(data.keys())}
            })

        df = data[self.timeframe]

        # ‚úÖ –î–û–ë–ê–í–ò–¢–¨ –≠–¢–û –õ–û–ì–ò–†–û–í–ê–ù–ò–ï
        self.logger.info(f"üîç ML DETECTOR DIAGNOSTIC:")
        self.logger.info(f"  DataFrame shape: {df.shape}")
        self.logger.info(f"  DataFrame type: {type(df)}")
        self.logger.info(f"  Index type: {type(df.index).__name__}")
        self.logger.info(f"  Index name: {df.index.name}")
        if hasattr(df.index, 'dtype'):
            self.logger.info(f"  Index dtype: {df.index.dtype}")
        self.logger.info(f"  Has 'ts' column: {'ts' in df.columns}")
        self.logger.info(f"  Has 'timestamp' column: {'timestamp' in df.columns}")
        self.logger.info(f"  Columns (first 15): {df.columns.tolist()[:15]}")

        # 3) –ü—Ä–æ–≤–µ—Ä–∫–∞ –Ω–∞ –ø—É—Å—Ç—ã–µ –¥–∞–Ω–Ω—ã–µ
        if df.empty:
            self.logger.error(f"‚ùå DataFrame for {self.timeframe} is empty")
            return normalize_signal({
                "ok": False,
                "direction": 0, #FLAT
                "confidence": 0.0,
                "reason": "empty_dataframe",
                "metadata": {"detector": "ml", "timeframe": self.timeframe}
            })

        # 4) –ù–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏—è –∫–æ–ª–æ–Ω–æ–∫ (ts -> timestamp)
        if 'ts' in df.columns and 'timestamp' not in df.columns:
            df = df.rename(columns={'ts': 'timestamp'})
            data[self.timeframe] = df  # –æ–±–Ω–æ–≤–ª—è–µ–º –≤ data

        # 5) –ü—Ä–æ–≤–µ—Ä–∫–∞ –æ–±—è–∑–∞—Ç–µ–ª—å–Ω—ã—Ö –∫–æ–ª–æ–Ω–æ–∫
        required_cols = ['open', 'high', 'low', 'close', 'volume']
        missing_cols = [col for col in required_cols if col not in df.columns]
        if missing_cols:
            self.logger.error(f"‚ùå Missing required columns: {missing_cols}")
            return normalize_signal({
                "ok": False,
                "direction": 0, #FLAT
                "confidence": 0.0,
                "reason": "missing_required_columns",
                "metadata": {"detector": "ml", "missing_cols": missing_cols}
            })

        # 6) Warmup
        if len(df) < self.required_warmup:
            self.logger.warning(f"‚ö†Ô∏è Insufficient data: {len(df)} < {self.required_warmup}")
            return normalize_signal({
                "ok": False,
                "direction": 0,
                "confidence": 0.0,
                "reason": "insufficient_warmup",
                "metadata": {
                    "detector": "ml",
                    "required": self.required_warmup,
                    "actual": len(df)
                }
            })
        else:
            # ‚úÖ –ù–û–í–û–ï: –õ–æ–≥–∏—Ä—É–µ–º –Ω–∞—á–∞–ª–æ –∞–Ω–∞–ª–∏–∑–∞
            self.logger.info(
                f"üéØ Starting ML analysis: {len(df)} candles available "
                f"(last candle ts={df['ts'].iloc[-1] if 'ts' in df.columns else 'N/A'})"
            )

        # 7) –ú–æ–¥–µ–ª—å –∑–∞–≥—Ä—É–∂–µ–Ω–∞?
        if self.model is None:
            self.logger.error("‚ùå Model not loaded! Call load_model() first.")
            return normalize_signal({
                "ok": False,
                "direction": 0, #FLAT
                "confidence": 0.0,
                "reason": "model_not_loaded",
                "metadata": {"detector": "ml"}
            })

        self.logger.info(f"‚úÖ All basic validations passed for {self.timeframe}")

        # ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
        # –ò–ó–í–õ–ï–ß–ï–ù–ò–ï –ü–†–ò–ó–ù–ê–ö–û–í
        # ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
        try:
            features = self.extract_features(df)
        except Exception as e:
            self.logger.error(f"‚ùå Feature extraction failed: {e}", exc_info=True)
            return normalize_signal({
                "ok": False,
                "direction": 0, #FLAT
                "confidence": 0.0,
                "reason": "feature_extraction_error",
                "metadata": {"detector": "ml", "error": str(e)}
            })

        # ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
        # –ú–ê–°–®–¢–ê–ë–ò–†–û–í–ê–ù–ò–ï
        # ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
        try:
            if self.use_scaler and self.scaler is not None:
                features_scaled = self.scaler.transform(features)
                self.logger.debug("üîç Using StandardScaler")
            else:
                features_scaled = features
                self.logger.debug("üîç Using RAW features")
        except Exception as e:
            self.logger.error(f"‚ùå Feature scaling failed: {e}")
            return normalize_signal({
                "ok": False,
                "direction": 0, #FLAT
                "confidence": 0.0,
                "reason": "scaling_error",
                "metadata": {"detector": "ml", "error": str(e)}
            })

        # ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
        # –ü–†–ï–î–°–ö–ê–ó–ê–ù–ò–ï
        # ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
        try:
            X_last = features_scaled[-1:].astype(np.float32)
            probabilities = self.model.predict(X_last)[0]

            # –û–±—Ä–∞–±–æ—Ç–∫–∞ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤
            flat_p, buy_p, sell_p = float(probabilities[0]), float(probabilities[1]), float(probabilities[2])

            prediction_idx = int(np.argmax(probabilities))
            predicted_class_confidence = float(probabilities[prediction_idx])

            self.last_confidence = predicted_class_confidence
            direction_map = {0: 0, 1: 1, 2: -1}
            predicted_direction = direction_map.get(prediction_idx, 0)

            predicted_class_confidence = float(probabilities[prediction_idx])

            # –ü—Ä–∏–º–µ–Ω–µ–Ω–∏–µ –ø–æ—Ä–æ–≥–∞ —É–≤–µ—Ä–µ–Ω–Ω–æ—Å—Ç–∏
            if predicted_direction == 0:
                ok = True
                reason = "no_trend_signal"
            else:  # BUY (1) –∏–ª–∏ SELL (-1)
                self.logger.info(
                    f"üîÑ ML –ø–æ—Ä–æ–≥: min_confidence={self.min_confidence} | conf={predicted_class_confidence:.3f} | "
                )
                ok = (predicted_class_confidence >= self.min_confidence)
                reason = "trend_confirmed" if ok else "weak_trend_signal"

            self.logger.info(
                f"üîÑ ML —Ä–µ–∑—É–ª—å—Ç–∞—Ç: {predicted_direction} | conf={predicted_class_confidence:.3f} | "
                f"BUY={buy_p:.3f} | SELL={sell_p:.3f} | FLAT={flat_p:.3f} | ok={ok} | reason={reason}"
            )

            return normalize_signal({
                "ok": ok,
                "direction": predicted_direction,
                "confidence": predicted_class_confidence,
                "reason": reason,
                "metadata": {
                    "detector": "ml",
                    "timeframe": self.timeframe,
                    "probabilities": {"FLAT": flat_p, "BUY": buy_p, "SELL": sell_p},
                    "predicted_class_confidence": predicted_class_confidence,
                    "feature_count": int(features.shape[1]),
                    "model_version": self.model_metadata.get("version", "unknown")
                }
            })

        except Exception as e:
            self.logger.error(f"‚ùå Prediction failed: {e}", exc_info=True)
            return normalize_signal({
                "ok": False,
                "direction": 0, #FLAT
                "confidence": 0.0,
                "reason": "prediction_error",
                "metadata": {"detector": "ml", "error": str(e)}
            })

    def load_model(self, path: str):
        """
        –ó–∞–≥—Ä—É–∑–∫–∞ –º–æ–¥–µ–ª–∏ —Å —É–ª—É—á—à–µ–Ω–Ω–æ–π –¥–∏–∞–≥–Ω–æ—Å—Ç–∏–∫–æ–π
        """
        if not os.path.exists(path):
            raise FileNotFoundError(f"Model file not found: {path}")

        try:
            self.logger.info(f"üîÑ –ó–∞–≥—Ä—É–∑–∫–∞ –º–æ–¥–µ–ª–∏ –∏–∑ {path}...")
            loaded_data = joblib.load(path)

            # –°–û–í–†–ï–ú–ï–ù–ù–´–ô –§–û–†–ú–ê–¢ (–∏–∑ trainer)
            if isinstance(loaded_data, dict):

                self.model = loaded_data.get("model")
                if self.model is None:
                    raise ValueError("Dictionary does not contain 'model' key")

                self.scaler = loaded_data.get("scaler")
                self.model_metadata = loaded_data.get("metadata", {})

                # –û–±–Ω–æ–≤–ª–µ–Ω–∏–µ –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤ –∏–∑ –º–æ–¥–µ–ª–∏
                self.timeframe = loaded_data.get("timeframe", self.timeframe)
                self.min_confidence = loaded_data.get("min_confidence", self.min_confidence)
                self.required_warmup = loaded_data.get("required_warmup", self.required_warmup)

                # –û–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è —Å–∫–µ–π–ª–µ—Ä–∞
                scaler_used = self.model_metadata.get("scaler_used", False)
                if hasattr(self, "use_scaler") and getattr(self, "use_scaler") is None:
                    self.use_scaler = scaler_used

                self.logger.info(
                    f"‚úÖ –ú–æ–¥–µ–ª—å –∑–∞–≥—Ä—É–∂–µ–Ω–∞: timeframe={self.timeframe}, "
                    f"scaler={'‚úì' if self.scaler else '‚úó'}, "
                    f"features={len(self.feature_names)}"
                )

            # LEGACY –§–û–†–ú–ê–¢
            elif isinstance(loaded_data, lgb.Booster):
                self.model = loaded_data
                self.scaler = None
                self.model_metadata = {
                    "version": "legacy",
                    "loaded_at": datetime.now(UTC).isoformat(),
                    "format": "raw_booster",
                    "scaler_used": False,
                }
                self.use_scaler = False
                self.logger.info("‚úÖ Legacy –º–æ–¥–µ–ª—å –∑–∞–≥—Ä—É–∂–µ–Ω–∞ (RAW features)")

            else:
                raise TypeError(f"Unsupported model format: {type(loaded_data)}")
            self.min_confidence = 0.53  # –ò–≥–Ω–æ—Ä–∏—Ä—É–µ–º –∑–Ω–∞—á–µ–Ω–∏–µ –∏–∑ –º–æ–¥–µ–ª–∏
            self.logger.info(f"üîß min_confidence overridden to {self.min_confidence}")
            # –í–ê–õ–ò–î–ê–¶–ò–Ø –ú–û–î–ï–õ–ò
            if not isinstance(self.model, lgb.Booster):
                raise TypeError(f"Model must be lgb.Booster, got {type(self.model).__name__}")

        except Exception as e:
            self.logger.error(f"‚ùå –û—à–∏–±–∫–∞ –∑–∞–≥—Ä—É–∑–∫–∏ –º–æ–¥–µ–ª–∏: {e}", exc_info=True)
            raise

    # ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê
    # –ú–ï–¢–û–î–´ –ò–ù–¢–ï–†–§–ï–ô–°–ê DETECTOR
    # ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê

    def get_required_bars(self) -> Dict[str, int]:
        """–ú–∏–Ω–∏–º–∞–ª—å–Ω–æ–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ –±–∞—Ä–æ–≤ –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞"""
        return {self.timeframe: self.required_warmup}


    def _validate_features(self, features: np.ndarray) -> bool:
        """
        –ü—Ä–æ–≤–µ—Ä—è–µ—Ç, —á—Ç–æ –º–∞—Å—Å–∏–≤ –ø—Ä–∏–∑–Ω–∞–∫–æ–≤ –Ω–µ —Å–æ–¥–µ—Ä–∂–∏—Ç NaN –∏–ª–∏ Inf.
        –í–æ–∑–≤—Ä–∞—â–∞–µ—Ç True, –µ—Å–ª–∏ –≤—Å—ë –∫–æ—Ä—Ä–µ–∫—Ç–Ω–æ, –∏–Ω–∞—á–µ False.
        """

        if features is None:
            self.logger.warning("[VALIDATOR] Features array is None")
            return False

        # –ü—Ä–æ–≤–µ—Ä–∫–∞ –Ω–∞ NaN –∏ Inf
        has_nan = np.isnan(features).any()
        has_inf = np.isinf(features).any()


        return not (has_nan or has_inf)
